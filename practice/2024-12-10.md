## Gradient decend
- Bolj splošno od nevronskih mrež
- Loss function ≃ error
- MAE -> mean absolute error

### Solution space
- 2D prostor, kjer lahko narišemo napake, in izberemo najboljšo fjo ven (vedno lahko obstaja boljša rešitev)
- Ne vemo kako izgledajo (poizkušaš doker ne najdeš najboljše rešitve)

## Back propagation
- Izvaja se na nevronskih mrežah
- Uteži: w = w + ∆w, ∆w = lr * error * f'(x) * input
- Bias: b = b + ∆b, ∆b = lr * error * f'(x)
- Back propagation -> nazaj propagiramo napake (napako za vsak layer izračunamo kot del napake iz prejšnjega layerja)
- Napake računamo z matrikami (transponirano mat. uteži množimo z matriko (vektorjem) napak)